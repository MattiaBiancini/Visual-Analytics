{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual Analytics\n",
    "\n",
    "## Assignment 3\n",
    "\n",
    "**Instructor:** Dr. Marco D'Ambros  \n",
    "**TAs:** Carmen Armenti, Mattia Giannaccari\n",
    "\n",
    "**Contacts:** marco.dambros@usi.ch, carmen.armenti@usi.ch, mattia.giannaccari@usi.ch\n",
    "\n",
    "**Due Date:** May 16, 2025 @ 23:55\n",
    "\n",
    "---\n",
    "The goal of this assignment is to use **Spark (PySpark)** and **Polars** in Jupyter notebooks.  \n",
    "The files `trip_data.csv`, `trip_fare.csv`, and `nyc_boroughs.geojson` are available in the provided folder: [Assignment3-data](https://usi365-my.sharepoint.com/:f:/g/personal/armenc_usi_ch/Ejp7sb8QAMROoWe0XUDcAkMBoqUFk-w2Vgroup025NhAww?e=2I7SMC).\n",
    "\n",
    "You may clean the data as needed; however, please note that specific data cleaning steps will be required in **Exercise 5**. If you choose to clean the data before Exercise 5, make sure to retain the **original dataset** for use with the Polars exercises.\n",
    "\n",
    "- Use **Spark** to solve **Exercises 1–4**\n",
    "- Use **Polars** to solve **Exercises 5–8**\n",
    "\n",
    "You are encouraged to use [Spark window functions](https://spark.apache.org/docs/latest/sql-ref-syntax-qry-select-window.html) whenever appropriate.\n",
    "\n",
    "Please name your notebook file as `SurnameName_Assignment3.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: polars in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (1.27.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting bokeh\n",
      "  Downloading bokeh-3.7.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: Jinja2>=2.9 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from bokeh) (3.1.6)\n",
      "Collecting contourpy>=1.2 (from bokeh)\n",
      "  Using cached contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting narwhals>=1.13 (from bokeh)\n",
      "  Downloading narwhals-1.37.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from bokeh) (1.26.4)\n",
      "Requirement already satisfied: packaging>=16.8 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from bokeh) (25.0)\n",
      "Requirement already satisfied: pandas>=1.2 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from bokeh) (2.2.3)\n",
      "Collecting pillow>=7.1.0 (from bokeh)\n",
      "  Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from bokeh) (6.0.2)\n",
      "Requirement already satisfied: tornado>=6.2 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from bokeh) (6.4.2)\n",
      "Collecting xyzservices>=2021.09.1 (from bokeh)\n",
      "  Downloading xyzservices-2025.4.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from Jinja2>=2.9->bokeh) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from pandas>=1.2->bokeh) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from pandas>=1.2->bokeh) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from pandas>=1.2->bokeh) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas>=1.2->bokeh) (1.17.0)\n",
      "Downloading bokeh-3.7.2-py3-none-any.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "Downloading narwhals-1.37.1-py3-none-any.whl (332 kB)\n",
      "Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Downloading xyzservices-2025.4.0-py3-none-any.whl (90 kB)\n",
      "Installing collected packages: xyzservices, pillow, narwhals, contourpy, bokeh\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [bokeh]━━━━━\u001b[0m \u001b[32m4/5\u001b[0m [bokeh]ls]\n",
      "\u001b[1A\u001b[2KSuccessfully installed bokeh-3.7.2 contourpy-1.3.2 narwhals-1.37.1 pillow-11.2.1 xyzservices-2025.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark\n",
    "%pip install polars\n",
    "%pip install bokeh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Join the `trip_data` and `trip_fare` dataframes into one and consider only data on 2013-01-01. Please specify the number of rows obtained after joining the 2 datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to create a `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "trip_data_df = spark.read.csv('./data/trip_data.csv', header=True, inferSchema=True)\n",
    "trip_fare_df = spark.read.csv('./data/trip_fare.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before joining the 2 dataframe I am going to filter the datas by the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|           medallion|        hack_license|vendor_id|rate_code|store_and_fwd_flag|    pickup_datetime|   dropoff_datetime|passenger_count|trip_time_in_secs|trip_distance|pickup_longitude|pickup_latitude|dropoff_longitude|dropoff_latitude|\n",
      "+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "|89D227B655E5C82AE...|BA96DE419E711691B...|      CMT|        1|                 N|2013-01-01 15:11:48|2013-01-01 15:18:10|              4|              382|          1.0|      -73.978165|      40.757977|       -73.989838|       40.751171|\n",
      "|0BD7C8F5BA12B88E0...|9FD8F69F0804BDB55...|      CMT|        1|                 N|2013-01-06 00:18:35|2013-01-06 00:22:54|              1|              259|          1.5|      -74.006683|      40.731781|       -73.994499|        40.75066|\n",
      "|0BD7C8F5BA12B88E0...|9FD8F69F0804BDB55...|      CMT|        1|                 N|2013-01-05 18:49:41|2013-01-05 18:54:23|              1|              282|          1.1|      -74.004707|       40.73777|       -74.009834|       40.726002|\n",
      "|DFD2202EE08F7A8DC...|51EE87E3205C985EF...|      CMT|        1|                 N|2013-01-07 23:54:15|2013-01-07 23:58:20|              2|              244|          0.7|      -73.974602|      40.759945|       -73.984734|       40.759388|\n",
      "|DFD2202EE08F7A8DC...|51EE87E3205C985EF...|      CMT|        1|                 N|2013-01-07 23:25:03|2013-01-07 23:34:24|              1|              560|          2.1|       -73.97625|      40.748528|       -74.002586|       40.747868|\n",
      "|20D9ECB2CA0767CF7...|598CCE5B9C1918568...|      CMT|        1|                 N|2013-01-07 15:27:48|2013-01-07 15:38:37|              1|              648|          1.7|      -73.966743|      40.764252|       -73.983322|       40.743763|\n",
      "|496644932DF393260...|513189AD756FF14FE...|      CMT|        1|                 N|2013-01-08 11:01:15|2013-01-08 11:08:14|              1|              418|          0.8|      -73.995804|      40.743977|       -74.007416|       40.744343|\n",
      "|0B57B9633A2FECD3D...|CCD4367B417ED6634...|      CMT|        1|                 N|2013-01-07 12:39:18|2013-01-07 13:10:56|              3|             1898|         10.7|      -73.989937|      40.756775|        -73.86525|        40.77063|\n",
      "|2C0E91FF20A856C89...|1DA2F6543A62B8ED9...|      CMT|        1|                 N|2013-01-07 18:15:47|2013-01-07 18:20:47|              1|              299|          0.8|      -73.980072|      40.743137|       -73.982712|       40.735336|\n",
      "|2D4B95E2FA7B2E851...|CD2F522EEE1FF5F5A...|      CMT|        1|                 N|2013-01-07 15:33:28|2013-01-07 15:49:26|              2|              957|          2.5|      -73.977936|      40.786983|       -73.952919|        40.80637|\n",
      "|E12F6AF991172EAC3...|06918214E951FA000...|      CMT|        1|                 N|2013-01-08 13:11:52|2013-01-08 13:19:50|              1|              477|          1.3|      -73.982452|      40.773167|       -73.964134|       40.773815|\n",
      "|E12F6AF991172EAC3...|06918214E951FA000...|      CMT|        1|                 N|2013-01-08 09:50:05|2013-01-08 10:02:54|              1|              768|          0.7|       -73.99556|      40.749294|       -73.988686|       40.759052|\n",
      "|78FFD9CD0CDA541F3...|E949C583ECF62C8F0...|      CMT|        1|                 N|2013-01-10 12:07:08|2013-01-10 12:17:29|              1|              620|          2.3|      -73.971497|      40.791321|       -73.964478|       40.775921|\n",
      "|237F49C3ECC11F502...|93C363DDF8ED9385D...|      CMT|        1|                 N|2013-01-07 07:35:47|2013-01-07 07:46:00|              1|              612|          2.3|       -73.98851|      40.774307|       -73.981094|       40.755325|\n",
      "|3349F919AA8AE5DC9...|7CE849FEF67514F08...|      CMT|        1|                 N|2013-01-10 15:42:29|2013-01-10 16:04:02|              1|             1293|          3.2|      -73.994911|      40.723221|       -73.971558|       40.761612|\n",
      "|3349F919AA8AE5DC9...|7CE849FEF67514F08...|      CMT|        1|                 N|2013-01-10 14:27:28|2013-01-10 14:45:21|              1|             1073|          4.4|      -74.010391|      40.708702|       -73.987846|       40.756104|\n",
      "|4C005EEBAA7BF26B8...|351BE7D984BE17DB2...|      CMT|        1|                 N|2013-01-07 22:09:59|2013-01-07 22:19:50|              1|              591|          1.7|      -73.973732|      40.756287|       -73.998413|       40.756832|\n",
      "|7D99C30FCE69B1A9D...|460C3F57DD9CB2265...|      CMT|        1|                 N|2013-01-07 17:18:16|2013-01-07 17:20:55|              1|              158|          0.7|      -73.968925|      40.767704|        -73.96199|       40.776566|\n",
      "|E6FBF80668FE0611A...|36773E80775F26CD1...|      CMT|        1|                 N|2013-01-07 06:08:51|2013-01-07 06:13:14|              1|              262|          1.7|       -73.96212|      40.769737|       -73.979561|        40.75539|\n",
      "|0C5296F3C8B16E702...|D2363240A9295EF57...|      CMT|        1|                 N|2013-01-07 22:25:46|2013-01-07 22:36:56|              1|              669|          2.3|      -73.989708|      40.756714|       -73.977615|       40.787575|\n",
      "+--------------------+--------------------+---------+---------+------------------+-------------------+-------------------+---------------+-----------------+-------------+----------------+---------------+-----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_data_df = trip_data_df.select([col(c).alias(c.strip()) for c in trip_data_df.columns])\n",
    "trip_data_df.printSchema()\n",
    "trip_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|           medallion|        hack_license|vendor_id|    pickup_datetime|payment_type|fare_amount|surcharge|mta_tax|tip_amount|tolls_amount|total_amount|\n",
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "|89D227B655E5C82AE...|BA96DE419E711691B...|      CMT|2013-01-01 15:11:48|         CSH|        6.5|      0.0|    0.5|       0.0|         0.0|         7.0|\n",
      "|0BD7C8F5BA12B88E0...|9FD8F69F0804BDB55...|      CMT|2013-01-06 00:18:35|         CSH|        6.0|      0.5|    0.5|       0.0|         0.0|         7.0|\n",
      "|0BD7C8F5BA12B88E0...|9FD8F69F0804BDB55...|      CMT|2013-01-05 18:49:41|         CSH|        5.5|      1.0|    0.5|       0.0|         0.0|         7.0|\n",
      "|DFD2202EE08F7A8DC...|51EE87E3205C985EF...|      CMT|2013-01-07 23:54:15|         CSH|        5.0|      0.5|    0.5|       0.0|         0.0|         6.0|\n",
      "|DFD2202EE08F7A8DC...|51EE87E3205C985EF...|      CMT|2013-01-07 23:25:03|         CSH|        9.5|      0.5|    0.5|       0.0|         0.0|        10.5|\n",
      "|20D9ECB2CA0767CF7...|598CCE5B9C1918568...|      CMT|2013-01-07 15:27:48|         CSH|        9.5|      0.0|    0.5|       0.0|         0.0|        10.0|\n",
      "|496644932DF393260...|513189AD756FF14FE...|      CMT|2013-01-08 11:01:15|         CSH|        6.0|      0.0|    0.5|       0.0|         0.0|         6.5|\n",
      "|0B57B9633A2FECD3D...|CCD4367B417ED6634...|      CMT|2013-01-07 12:39:18|         CSH|       34.0|      0.0|    0.5|       0.0|         4.8|        39.3|\n",
      "|2C0E91FF20A856C89...|1DA2F6543A62B8ED9...|      CMT|2013-01-07 18:15:47|         CSH|        5.5|      1.0|    0.5|       0.0|         0.0|         7.0|\n",
      "|2D4B95E2FA7B2E851...|CD2F522EEE1FF5F5A...|      CMT|2013-01-07 15:33:28|         CSH|       13.0|      0.0|    0.5|       0.0|         0.0|        13.5|\n",
      "|E12F6AF991172EAC3...|06918214E951FA000...|      CMT|2013-01-08 13:11:52|         CSH|        7.5|      0.0|    0.5|       0.0|         0.0|         8.0|\n",
      "|E12F6AF991172EAC3...|06918214E951FA000...|      CMT|2013-01-08 09:50:05|         CSH|        9.0|      0.0|    0.5|       0.0|         0.0|         9.5|\n",
      "|78FFD9CD0CDA541F3...|E949C583ECF62C8F0...|      CMT|2013-01-10 12:07:08|         CSH|        9.5|      0.0|    0.5|       0.0|         0.0|        10.0|\n",
      "|237F49C3ECC11F502...|93C363DDF8ED9385D...|      CMT|2013-01-07 07:35:47|         CSH|       10.0|      0.0|    0.5|       0.0|         0.0|        10.5|\n",
      "|3349F919AA8AE5DC9...|7CE849FEF67514F08...|      CMT|2013-01-10 15:42:29|         CSH|       15.5|      0.0|    0.5|       0.0|         0.0|        16.0|\n",
      "|3349F919AA8AE5DC9...|7CE849FEF67514F08...|      CMT|2013-01-10 14:27:28|         CSH|       16.5|      0.0|    0.5|       0.0|         0.0|        17.0|\n",
      "|4C005EEBAA7BF26B8...|351BE7D984BE17DB2...|      CMT|2013-01-07 22:09:59|         CSH|        9.0|      0.5|    0.5|       0.0|         0.0|        10.0|\n",
      "|7D99C30FCE69B1A9D...|460C3F57DD9CB2265...|      CMT|2013-01-07 17:18:16|         CSH|        4.5|      1.0|    0.5|       0.0|         0.0|         6.0|\n",
      "|E6FBF80668FE0611A...|36773E80775F26CD1...|      CMT|2013-01-07 06:08:51|         CSH|        7.0|      0.0|    0.5|       0.0|         0.0|         7.5|\n",
      "|0C5296F3C8B16E702...|D2363240A9295EF57...|      CMT|2013-01-07 22:25:46|         CSH|       10.5|      0.5|    0.5|       0.0|         0.0|        11.5|\n",
      "+--------------------+--------------------+---------+-------------------+------------+-----------+---------+-------+----------+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "trip_fare_df = trip_fare_df.select([col(c).alias(c.strip()) for c in trip_fare_df.columns])\n",
    "trip_fare_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 2 temporal metric I am going to filter on both in order to avoid `pickup_datetime` being 2012-12-31 while `dropoff_datetime` is 2013-01-01 and in similar way I want to avoid `pickup_datetime` being 2013-01-01 while `dropoff_datetime` is 2013-01-02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data_filtered_df = trip_data_df.filter(\n",
    "    (to_date(\"pickup_datetime\") == \"2013-01-01\") &\n",
    "    (to_date(\"dropoff_datetime\") == \"2013-01-01\")\n",
    ")\n",
    "\n",
    "trip_fare_filtered_df = trip_fare_df.filter(\n",
    "    (to_date(\"pickup_datetime\") == \"2013-01-01\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can now merge the 2 dataframes using the `join` per `medallion`, `hack_license`, `vendor_id` and `pickup_datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_filtered_df = trip_data_filtered_df.join(\n",
    "    trip_fare_filtered_df,\n",
    "    on=[\"medallion\", \"hack_license\", \"vendor_id\", \"pickup_datetime\"],\n",
    "    how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 120:==============================================>        (11 + 2) / 13]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows are: 410816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(f\"The number of rows are: {trip_filtered_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Provide a graphical representation to compare the average fare amount for trips _within_ and _across_ all the boroughs. You may want to have a look at: https://docs.bokeh.org/en/latest/docs/user_guide/topics/categorical.html#categorical-heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- medallion: string (nullable = true)\n",
      " |-- hack_license: string (nullable = true)\n",
      " |-- vendor_id: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- rate_code: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_time_in_secs: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- pickup_longitude: double (nullable = true)\n",
      " |-- pickup_latitude: double (nullable = true)\n",
      " |-- dropoff_longitude: double (nullable = true)\n",
      " |-- dropoff_latitude: double (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- surcharge: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Consider only Manhattan, Bronx and Brooklyn boroughs. Then create a dataframe that shows the total number of trips *within* the same borough and *across* all the other boroughs mentioned before (Manhattan, Bronx, and Brooklyn) where the passengers are more or equal than 3.\n",
    "\n",
    "For example, for Manhattan borough you should consider the total number of the following trips:\n",
    "- Manhattan → Manhattan\n",
    "- Manhattan → Bronx\n",
    "- Manhattan → Brooklyn\n",
    "\n",
    "You should then do the same for Bronx and Brooklyn boroughs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Create a dataframe where each row represents a driver, and there is one column per borough.\n",
    "For each driver-borough, the dataframe provides the maximum number of consecutive trips\n",
    "for the given driver, within the given borough. Please consider only trips which were payed by card. \n",
    "\n",
    "For example, if for driver A we have (sorted by time):\n",
    "- Trip 1: Bronx → Bronx\n",
    "- Trip 2: Bronx → Bronx\n",
    "- Trip 3: Bronx → Manhattan\n",
    "- Trip 4: Manhattan → Bronx.\n",
    "    \n",
    "The maximum number of consecutive trips for Bronx is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "Please work on the merged dataset of trips and fares and perform the following data cleaning tasks:\n",
    "\n",
    "1. Remove trips with invalid locations (i.e. not in New York City);\n",
    "3. Remove trips with invalid amounts:\n",
    "    - Total amount must be greater than zero;\n",
    "    - Total amount must correspond to the sum of all the other amounts.\n",
    "5. Remove trips with invalid time:\n",
    "    - Pick-up before drop-off;\n",
    "    - Valid duration.\n",
    "\n",
    "After each data cleaning task, report how many rows where removed. Finally report:\n",
    "- Are there **duplicate trips**?\n",
    "- How many trips remain after cleaning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected encoding: ascii\n"
     ]
    }
   ],
   "source": [
    "with open('data/trip_fare.csv', 'rb') as f:\n",
    "    data = f.read()\n",
    "\n",
    "encoding_result = chardet.detect(data)\n",
    "encoding = encoding_result['encoding']\n",
    "print(f\"Detected encoding: {encoding}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputeError",
     "evalue": "could not parse `1.75` as dtype `i64` at column ' tip_amount' (column number 9)\n\nThe current offset in the file is 17600 bytes.\n\nYou might want to try:\n- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),\n- specifying correct dtype with the `schema_overrides` argument\n- setting `ignore_errors` to `True`,\n- adding `1.75` to the `null_values` list.\n\nOriginal error: ```remaining bytes non-empty```",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mComputeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m tripdata_polar_df = pl.read_csv(\u001b[33m'\u001b[39m\u001b[33m./data/trip_data.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m tripfare_polar_df = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data/trip_fare.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages/polars/_utils/deprecation.py:119\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    116\u001b[39m     _rename_keyword_argument(\n\u001b[32m    117\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    118\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages/polars/_utils/deprecation.py:119\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    116\u001b[39m     _rename_keyword_argument(\n\u001b[32m    117\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    118\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages/polars/_utils/deprecation.py:119\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    116\u001b[39m     _rename_keyword_argument(\n\u001b[32m    117\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    118\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages/polars/io/csv/functions.py:539\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(source, has_header, columns, new_columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    532\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m prepare_file_arg(\n\u001b[32m    533\u001b[39m         source,\n\u001b[32m    534\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    537\u001b[39m         storage_options=storage_options,\n\u001b[32m    538\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m         df = \u001b[43m_read_csv_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    540\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    544\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnull_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnull_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf8-lossy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m            \u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m            \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m new_columns:\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _update_columns(df, new_columns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/VS-assignment3/lib/python3.12/site-packages/polars/io/csv/functions.py:687\u001b[39m, in \u001b[36m_read_csv_impl\u001b[39m\u001b[34m(source, has_header, columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[39m\n\u001b[32m    683\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    685\u001b[39m projection, columns = parse_columns_arg(columns)\n\u001b[32m--> \u001b[39m\u001b[32m687\u001b[39m pydf = \u001b[43mPyDataFrame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    688\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    689\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    690\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    691\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    692\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    693\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_null_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_row_index_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(pydf)\n",
      "\u001b[31mComputeError\u001b[39m: could not parse `1.75` as dtype `i64` at column ' tip_amount' (column number 9)\n\nThe current offset in the file is 17600 bytes.\n\nYou might want to try:\n- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),\n- specifying correct dtype with the `schema_overrides` argument\n- setting `ignore_errors` to `True`,\n- adding `1.75` to the `null_values` list.\n\nOriginal error: ```remaining bytes non-empty```"
     ]
    }
   ],
   "source": [
    "tripdata_polar_df = pl.read_csv('./data/trip_data.csv')\n",
    "tripfare_polar_df = pl.read_csv('./data/trip_fare.csv', encoding=encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "Compute the **total revenue** (total_amount) grouped by:\n",
    "- Pick-up hour of the day (0–23)\n",
    "- Passenger count (group >=6 into “6+”)\n",
    "\n",
    "Create a heatmap where:\n",
    "- X-axis = hour\n",
    "- Y-axis = passenger count group\n",
    "- Cell value = average revenue per trip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "Define an \"anomalous trip\" as one that satisfies at least two of the following:\n",
    "- Fare per mile is above the 95th percentile\n",
    "- Tip amount > 100% of fare\n",
    "- trip_time_in_secs is less than 60 seconds but distance is more than 1 mile\n",
    "\n",
    "Create a dataframe of anomalous trips and:\n",
    "- Report how many such trips exist\n",
    "- Create a scatterplot to visualize the anomaly metrics\n",
    "- Describe the visualization identifying groups and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "For each driver (hack_license), calculate the **total profit per hour worked**, where:\n",
    "> profit = 0.7 * (fare_amount + tip_amount) when the trip starts between 7:01 AM and 7:00 PM\\\n",
    "> profit = 0.8 * (fare_amount + tip_amount) when the trip starts between 7:01PM and 7:00 AM\n",
    "\n",
    "Estimate \"hours worked\" by summing trip_time_in_secs.\n",
    "\n",
    "Plot a line chart showing the distribution of average profit per hour **for the top 10% drivers** in terms of total trips.\n",
    "\n",
    "Which time of day offers **best earning efficiency**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
